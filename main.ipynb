{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6f0e1aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, pipeline, TrainingArguments, Trainer\n",
    "from datasets import Dataset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import stanza"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce22ed2e",
   "metadata": {},
   "source": [
    "## Load and split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5264ddf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, ids = [], []\n",
    "with open('train_reviews.txt') as f:\n",
    "    for line in f:\n",
    "        text_id, text = line.rstrip('\\r\\n').split('\\t')\n",
    "        texts.append(text)\n",
    "        ids.append(text_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8e07138e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, dev_texts, train_ids, dev_ids = train_test_split(texts, ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "691e6ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_aspects, dev_aspects = [], []\n",
    "with open('train_aspects.txt') as f:\n",
    "    for line in f:\n",
    "        line = line.rstrip('\\r\\n')\n",
    "        text_id = line.split('\\t')[0]\n",
    "        if text_id in train_ids:\n",
    "            train_aspects.append(line)\n",
    "        if text_id in dev_ids:\n",
    "            dev_aspects.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "23579b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentiment, dev_sentiment = [], []\n",
    "with open('train_cats.txt') as f:\n",
    "    for line in f:\n",
    "        line = line.rstrip('\\r\\n')\n",
    "        text_id = line.split('\\t')[0]\n",
    "        if text_id in train_ids:\n",
    "            train_sentiment.append(line)\n",
    "        if text_id in dev_ids:\n",
    "            dev_sentiment.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e30d5b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_split_aspects.txt', 'w') as f:\n",
    "    for l in train_aspects:\n",
    "        print(l, file=f)\n",
    "with open('dev_aspects.txt', 'w') as f:\n",
    "    for l in dev_aspects:\n",
    "        print(l, file=f)\n",
    "with open('train_split_reviews.txt', 'w') as f:\n",
    "    for i, l in zip(train_ids, train_texts):\n",
    "        print(i, l, sep=\"\\t\", file=f)\n",
    "with open('dev_reviews.txt', 'w') as f:\n",
    "    for i, l in zip(dev_ids, dev_texts):\n",
    "        print(i, l, sep=\"\\t\", file=f)\n",
    "with open('train_split_cats.txt', 'w') as f:\n",
    "    for l in train_sentiment:\n",
    "        print(l, file=f)\n",
    "with open('dev_cats.txt', 'w') as f:\n",
    "    for l in dev_sentiment:\n",
    "        print(l, file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11a0795",
   "metadata": {},
   "source": [
    "## Prepare aspects dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ac8d2a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for elem in train_aspects:\n",
    "    cur = list(elem.split('\\t'))\n",
    "    row = {\n",
    "        'token': cur[2],\n",
    "        'category': cur[1],\n",
    "    }\n",
    "    data.append(row)\n",
    "\n",
    "asp_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "09bfdd53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Food        1426\n",
       "Service      927\n",
       "Whole        604\n",
       "Interior     516\n",
       "Price        100\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asp_df.category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "54d2f2e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b158dff09bc141bdb59da91fbe218608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-17 14:24:27 INFO: Downloading default packages for language: ru (Russian) ...\n",
      "2023-01-17 14:24:28 INFO: File exists: /Users/ceo/stanza_resources/ru/default.zip\n",
      "2023-01-17 14:24:31 INFO: Finished downloading models and saved to /Users/ceo/stanza_resources.\n",
      "2023-01-17 14:24:31 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c35d6e441554d0d855511dd54aa6c48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.4.1.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-17 14:24:32 INFO: Loading these models for language: ru (Russian):\n",
      "=========================\n",
      "| Processor | Package   |\n",
      "-------------------------\n",
      "| tokenize  | syntagrus |\n",
      "| lemma     | syntagrus |\n",
      "=========================\n",
      "\n",
      "2023-01-17 14:24:32 INFO: Use device: cpu\n",
      "2023-01-17 14:24:32 INFO: Loading: tokenize\n",
      "2023-01-17 14:24:32 INFO: Loading: lemma\n",
      "2023-01-17 14:24:32 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "stanza.download('ru')\n",
    "\n",
    "stanza_nlp = stanza.Pipeline('ru', processors='tokenize,lemma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d6a657be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    doc = stanza_nlp(text)\n",
    "    words = [word.lemma for sent in doc.sentences for word in sent.words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "58537fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_tokens = []\n",
    "for elem in asp_df.iterrows():\n",
    "    norm_tokens.append(normalize(elem[1]['token']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bf98e236",
   "metadata": {},
   "outputs": [],
   "source": [
    "asp_df['norm'] = norm_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3678d969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>category</th>\n",
       "      <th>norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ресторане</td>\n",
       "      <td>Whole</td>\n",
       "      <td>[ресторан]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ресторанах</td>\n",
       "      <td>Whole</td>\n",
       "      <td>[ресторан]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ресторане</td>\n",
       "      <td>Whole</td>\n",
       "      <td>[ресторан]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Столик бронировали</td>\n",
       "      <td>Service</td>\n",
       "      <td>[столик, бронировали]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>администратор</td>\n",
       "      <td>Service</td>\n",
       "      <td>[администратор]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                token category                   norm\n",
       "0           ресторане    Whole             [ресторан]\n",
       "1          ресторанах    Whole             [ресторан]\n",
       "2           ресторане    Whole             [ресторан]\n",
       "3  Столик бронировали  Service  [столик, бронировали]\n",
       "4       администратор  Service        [администратор]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asp_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f65e267",
   "metadata": {},
   "source": [
    "## Aspects extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "870b0481",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"ru_core_news_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3d7536d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "71it [00:02, 24.32it/s]\n"
     ]
    }
   ],
   "source": [
    "aspects = []\n",
    "start_pos = []\n",
    "end_pos = []\n",
    "descr = []\n",
    "asp_ids = []\n",
    "\n",
    "for t, idx in tqdm(zip(dev_texts, dev_ids)):\n",
    "    doc = nlp(t)\n",
    "\n",
    "    for token in doc:\n",
    "        if token.pos_ == 'NOUN':\n",
    "            for j in token.lefts:\n",
    "                if j.dep_ == 'amod' and j.pos_ == 'ADJ':\n",
    "                    flag = False\n",
    "                    for k in j.lefts:\n",
    "                        if k.dep_ == 'advmod':\n",
    "                            flag = True\n",
    "                            aspects.append(token)\n",
    "                            start_pos.append(token.idx)\n",
    "                            end_pos.append(token.idx + len(token))\n",
    "                            descr.append(k.lemma_ + ' ' + j.lemma_)\n",
    "                            asp_ids.append(idx)\n",
    "                            \n",
    "                    if not flag:\n",
    "                        aspects.append(token)\n",
    "                        start_pos.append(token.idx)\n",
    "                        end_pos.append(token.idx + len(token))\n",
    "                        descr.append(j.lemma_)\n",
    "                        asp_ids.append(idx)\n",
    "                    \n",
    "        if token.pos_ == 'VERB':\n",
    "            for j in token.lefts:\n",
    "                if j.dep_ == 'advmod' and j.pos_ == 'ADV':\n",
    "                    aspects.append(token)\n",
    "                    start_pos.append(token.idx)\n",
    "                    end_pos.append(token.idx + len(token))\n",
    "                    descr.append(j.lemma_)\n",
    "                    asp_ids.append(idx)\n",
    "                    \n",
    "            for j in token.rights:\n",
    "                if j.dep_ == 'advmod'and j.pos_ == 'ADV':\n",
    "                    aspects.append(token)\n",
    "                    start_pos.append(token.idx)\n",
    "                    end_pos.append(token.idx + len(token))\n",
    "                    descr.append(j.lemma_)\n",
    "                    asp_ids.append(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d97687",
   "metadata": {},
   "source": [
    "## Aspects classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dce0eeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {}\n",
    "\n",
    "label2id = {\n",
    "    'Food': 0,\n",
    "    'Whole': 1,\n",
    "    'Service': 2,\n",
    "    'Interior': 3,\n",
    "    'Price': 4,\n",
    "}\n",
    "\n",
    "id2label = {\n",
    "    0: 'Food',\n",
    "    1: 'Whole',\n",
    "    2: 'Service',\n",
    "    3: 'Interior',\n",
    "    4: 'Price',\n",
    "    5: 'None',\n",
    "}\n",
    "\n",
    "for elem in asp_df.iterrows():\n",
    "    for lemm in elem[1]['norm']:\n",
    "        if lemm not in weights:\n",
    "            weights[lemm] = {\n",
    "                0: 0,\n",
    "                1: 0,\n",
    "                2: 0,\n",
    "                3: 0,\n",
    "                4: 0,\n",
    "            }\n",
    "        \n",
    "        weights[lemm][label2id[elem[1]['category']]] += 1\n",
    "        \n",
    "for lemm in weights:\n",
    "    total = weights[lemm][0] + weights[lemm][1] + weights[lemm][2] + weights[lemm][3] + weights[lemm][4]\n",
    "    weights[lemm][0] /= total\n",
    "    weights[lemm][1] /= total\n",
    "    weights[lemm][2] /= total\n",
    "    weights[lemm][3] /= total\n",
    "    weights[lemm][4] /= total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3dbc608a",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "\n",
    "for token in aspects:\n",
    "    lemm = token.lemma_\n",
    "    if lemm not in weights:\n",
    "        labels.append('None')\n",
    "    else:\n",
    "        cur_label = 5\n",
    "        best = 0\n",
    "        for i in range(5):\n",
    "            if weights[lemm][i] > 0.4 and weights[lemm][i] > best:\n",
    "                best = weights[lemm][i]\n",
    "                cur_label = i\n",
    "                \n",
    "        labels.append(id2label[cur_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4e13e50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_asp = pd.DataFrame(\n",
    "    {\n",
    "        'text_id': asp_ids,\n",
    "        'category': labels,\n",
    "        'mention': aspects,\n",
    "        'start': start_pos,\n",
    "        'end': end_pos,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5f48d7",
   "metadata": {},
   "source": [
    "## Aspect sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9876a060",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_pipe = pipeline(\"sentiment-analysis\", model='Tatyana/rubert-base-cased-sentiment-new')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1901e5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1034/1034 [00:39<00:00, 26.08it/s]\n"
     ]
    }
   ],
   "source": [
    "sent_scores = []\n",
    "sent_labels = []\n",
    "\n",
    "for elem in tqdm(descr):\n",
    "    res = sentiment_pipe(elem)\n",
    "    sent_scores.append(res[0]['score'])\n",
    "    sent_labels.append(res[0]['label'].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f3508654",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_asp['sentiment'] = sent_labels\n",
    "pred_asp['sent_score'] = sent_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5411aa",
   "metadata": {},
   "source": [
    "## Aspects finish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a075eec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_asp.drop(pred_asp[pred_asp['category'] == 'None'].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "68e70766",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_id</th>\n",
       "      <th>category</th>\n",
       "      <th>mention</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sent_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16568</td>\n",
       "      <td>Service</td>\n",
       "      <td>посетителем</td>\n",
       "      <td>87</td>\n",
       "      <td>98</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.810643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16568</td>\n",
       "      <td>Whole</td>\n",
       "      <td>мест</td>\n",
       "      <td>172</td>\n",
       "      <td>176</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.979600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16568</td>\n",
       "      <td>Food</td>\n",
       "      <td>еда</td>\n",
       "      <td>254</td>\n",
       "      <td>257</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.976750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>16568</td>\n",
       "      <td>Food</td>\n",
       "      <td>рыба</td>\n",
       "      <td>276</td>\n",
       "      <td>280</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.749643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>16568</td>\n",
       "      <td>Food</td>\n",
       "      <td>рыба</td>\n",
       "      <td>276</td>\n",
       "      <td>280</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.748429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  text_id category      mention  start  end sentiment  sent_score\n",
       "1   16568  Service  посетителем     87   98   neutral    0.810643\n",
       "2   16568    Whole         мест    172  176  positive    0.979600\n",
       "4   16568     Food          еда    254  257  positive    0.976750\n",
       "5   16568     Food         рыба    276  280  negative    0.749643\n",
       "6   16568     Food         рыба    276  280   neutral    0.748429"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_asp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "30c5f20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('final_pred_aspects.txt', 'w') as f:\n",
    "    for elem in pred_asp.drop(columns='sent_score', axis=1).iterrows():\n",
    "        vals = list(map(str, elem[1].values))\n",
    "        print('\\t'.join(vals), file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d15b5a",
   "metadata": {},
   "source": [
    "## Text sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a504c777",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pd/zt6wkns9337cdlsf7rmzl8d40000gn/T/ipykernel_39247/1498168376.py:15: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  for elem in pred_asp[pred_asp['text_id'] == text_id][pred_asp['category'] == cat].iterrows():\n"
     ]
    }
   ],
   "source": [
    "CATEGORIES = ['Whole', 'Interior', 'Service', 'Food', 'Price']\n",
    "\n",
    "sent_ids = []\n",
    "sent_cats = []\n",
    "text_sent = []\n",
    "\n",
    "for text_id in dev_ids:\n",
    "    for cat in CATEGORIES:\n",
    "        positive = 0\n",
    "        negative = 0\n",
    "        neutral = 0\n",
    "        cnt_pos = 0\n",
    "        cnt_neg = 0\n",
    "        cnt_neu = 0\n",
    "        for elem in pred_asp[pred_asp['text_id'] == text_id][pred_asp['category'] == cat].iterrows():\n",
    "            if elem[1].sentiment == 'positive':\n",
    "                positive += elem[1].sent_score * elem[1].sent_score\n",
    "                cnt_pos += 1\n",
    "            elif elem[1].sentiment == 'negative':\n",
    "                negative += elem[1].sent_score * elem[1].sent_score\n",
    "                cnt_neg += 1\n",
    "            else:\n",
    "                neutral += elem[1].sent_score * elem[1].sent_score\n",
    "                cnt_neu += 1\n",
    "            \n",
    "        if cnt_pos > 1 and cnt_neg > 1:\n",
    "            verdict = 'both'\n",
    "        elif cnt_pos + cnt_neg + cnt_neu == 0:\n",
    "            verdict = 'absence'\n",
    "        else:\n",
    "            verdict = 'positive'\n",
    "            best = positive\n",
    "            \n",
    "            if negative > best:\n",
    "                best = negative\n",
    "                verdict = 'negative'\n",
    "            \n",
    "            if neutral > best:\n",
    "                best = neutral\n",
    "                verdict = 'neutral'\n",
    "        \n",
    "        sent_ids.append(text_id)\n",
    "        sent_cats.append(cat)\n",
    "        text_sent.append(verdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "06ca7a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred_cats = pd.DataFrame(\n",
    "    {\n",
    "        'text_id': sent_ids,\n",
    "        'category': sent_cats,\n",
    "        'sentiment': text_sent,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "edce9fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('final_pred_cats.txt', 'w') as f:\n",
    "    for elem in final_pred_cats.iterrows():\n",
    "        vals = list(map(str, elem[1].values))\n",
    "        print('\\t'.join(vals), file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b128a6",
   "metadata": {},
   "source": [
    "## Отчет"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16d0b0d",
   "metadata": {},
   "source": [
    "### Использованные методы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942d8cc0",
   "metadata": {},
   "source": [
    "* Для выделения aspects и opinions была использована библиотека spacy\n",
    "* Для классификации аспектов планировалось применение bert-base-multilingual-cased + fine-tune с помощью библиотеки transformers от hugging-face, но к сожалению она быстро не завелась (а время сильно поджимало) и пришлось использовать dummy-классификацию\n",
    "* Для оценки sentiment скора была применена предобученная модель Tatyana/rubert-base-cased-sentiment-new с помощью библиотеки transformers\n",
    "* text/aspect окрас был предсказан на основе полученных opinions при выделении аспектов с помощью скора sentiment анализа по аспектам\n",
    "* Для лемматизации также кое-где была использована stanza"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791ee925",
   "metadata": {},
   "source": [
    "### Потенциальные лучшения"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b02999",
   "metadata": {},
   "source": [
    "* Самое тривиальное, но действенное – улучшить выбор кандидатов в список тематических аспектов. Необходимо добавить больше синтаксических паттернов в разбор выражений\n",
    "* Как было описано выше – использовать предобученную модель + fine-tune для классификации аспектов\n",
    "* Аналогично, можно сделать fine-tune для модели оценки окраса (в нашем случае это tatyana/rubert-base-cased-sentiment-new)\n",
    "* Еще один простой способ повысить скор – смержить бейзлайн модель с нашей (что не стало делаться из уважения, вместо этого были попытки успеть завести качественную модель)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af10e70",
   "metadata": {},
   "source": [
    "### Инструкция для тестирования"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a07f3b6",
   "metadata": {},
   "source": [
    "Данный файл содержит в себе весь пайплайн. Для тестирования на других данных но при той же обучающей выборке нужно подставить необходимые dev_texts и dev_ids.\n",
    "Оценка скора производится скриптом из условия."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd5e60c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
